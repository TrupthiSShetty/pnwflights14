{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import *\n",
    "from keras.losses import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "from keras.activations import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\HPPC\\train.tsv\", sep='\\t')\n",
    "test = pd.read_csv(r\"C:\\Users\\HPPC\\test.tsv\", sep='\\t')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessing_func():\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def preprocessing_func(sent):\n",
    "        return [lemmatizer.lemmatize(w) for w in tokenizer.tokenize(sent)]\n",
    "    return preprocessing_func\n",
    "\n",
    "X = train['Phrase'].apply(get_preprocessing_func()).values\n",
    "y = train['Sentiment'].values\n",
    "X_test = test['Phrase'].apply(get_preprocessing_func()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tokenizer_and_weights(X):\n",
    "    tokenizer = Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    \n",
    "    weights = np.zeros((len(tokenizer.word_index)+1, 300))\n",
    "    with open(r\"C:\\Users\\HPPC\\crawl-300d-2M.vec\",encoding ='utf-8') as f:\n",
    "        next(f)\n",
    "        for l in f:\n",
    "            w = l.split(' ')\n",
    "            if w[0] in tokenizer.word_index:\n",
    "                weights[tokenizer.word_index[w[0]]] = np.array([float(x) for x in w[1:301]])\n",
    "    return tokenizer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_LEN= 64\n",
      "MAX_ID= 16445\n"
     ]
    }
   ],
   "source": [
    "tokenizer, weights = prepare_tokenizer_and_weights(np.append(X, X_test))\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "MAX_LEN = max(map(lambda x: len(x), X_seq))\n",
    "X_seq = pad_sequences(X_seq, MAX_LEN)\n",
    "MAX_ID = len(tokenizer.word_index)\n",
    "print('MAX_LEN=', MAX_LEN)\n",
    "print('MAX_ID=', MAX_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 64, 300)           4933800   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_13 (Spatia (None, 64, 300)           0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_13 (Glo (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5)                 1505      \n",
      "=================================================================\n",
      "Total params: 4,935,305\n",
      "Trainable params: 4,935,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/1\n",
      " - 2211s - loss: 1.1777 - acc: 0.5295 - val_loss: 0.9529 - val_acc: 0.6150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/1\n",
      " - 2150s - loss: 1.1823 - acc: 0.5288 - val_loss: 0.9789 - val_acc: 0.5896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/1\n",
      " - 2384s - loss: 1.1739 - acc: 0.5308 - val_loss: 0.9680 - val_acc: 0.5996\n"
     ]
    }
   ],
   "source": [
    "def make_fast_text():\n",
    "    fast_text = Sequential()\n",
    "    fast_text.add(InputLayer((MAX_LEN,))) \n",
    "    fast_text.add(Embedding(input_dim=MAX_ID+1, output_dim=300, weights=[weights], trainable=True))\n",
    "    fast_text.add(SpatialDropout1D(0.5))\n",
    "    fast_text.add(GlobalMaxPooling1D())\n",
    "    fast_text.add(Dropout(0.5))\n",
    "    fast_text.add(Dense(5,activation='softmax'))\n",
    "    return fast_text\n",
    "\n",
    "fast_texts = [make_fast_text() for i in range(3)]\n",
    "fast_texts[0].summary()\n",
    "\n",
    "for fast_text in fast_texts:\n",
    "    X_seq_train, X_seq_valid, y_train, y_valid = train_test_split(X_seq, y, test_size=0.1)\n",
    "    fast_text.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    fast_text.fit(X_seq_train, y_train, validation_data=(X_seq_valid, y_valid),\n",
    "                 callbacks=[EarlyStopping(monitor='val_loss', patience=2, verbose=0)],\n",
    "                 epochs=1, \n",
    "                 verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      7072\n",
      "           1       0.49      0.31      0.38     27273\n",
      "           2       0.64      0.93      0.76     79582\n",
      "           3       0.55      0.38      0.45     32927\n",
      "           4       0.74      0.03      0.07      9206\n",
      "\n",
      "   micro avg       0.61      0.61      0.61    156060\n",
      "   macro avg       0.48      0.33      0.33    156060\n",
      "weighted avg       0.57      0.61      0.55    156060\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_prob = fast_text.predict(X_seq)\n",
    "y_predict = np.argmax(y_prob, axis=1)\n",
    "print(classification_report(y, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
